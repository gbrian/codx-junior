"""
This module provides an interface to interact with AI models.
It leverages the OpenAI GPT models and allows for integration with Azure-based instances of the same.
The AI class encapsulates the chat functionalities, allowing to start, advance, and manage a conversation with the model.

Key Features:
- Integration with Azure-based OpenAI instances through the LangChain AzureChatOpenAI class.
- Token usage logging to monitor the number of tokens consumed during a conversation.
- Seamless fallback to default models in case the desired model is unavailable.
- Serialization and deserialization of chat messages for easier transmission and storage.

Classes:
- AI: Main class providing chat functionalities.

Dependencies:
- langchain: For chat models and message schemas.
- openai: For the core GPT models interaction.
- backoff: For handling rate limits and retries.
- typing: For type hints.

For more specific details, refer to the docstrings within each class and function.
"""

from __future__ import annotations

import json
import logging
import os

from typing import List, Optional, Union

import backoff
import openai

import hashlib


from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

from langchain_community.chat_models import ChatOpenAI
from langchain.chat_models.base import BaseChatModel
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    messages_from_dict,
    messages_to_dict,
)

from codx.junior.settings import GPTEngineerSettings
from codx.junior.ai.openai_ai import OpenAI_AI

# Type hint for a chat message
Message = Union[AIMessage, HumanMessage, SystemMessage]

# Set up logging
logger = logging.getLogger(__name__)


class LogginCallbackHandler(StreamingStdOutCallbackHandler):
    pass

class AI:
    """
    A class to interface with a language model for chat-based interactions.

    This class provides methods to initiate and maintain conversations using
    a specified language model. It handles token counting, message creation,
    serialization and deserialization of chat messages, and interfaces with
    the language model to get AI-generated responses.

    Attributes
    ----------
    temperature : float
        The temperature setting for the model, affecting the randomness of the output.
    azure_endpoint : str
        The Azure endpoint URL, if applicable.
    model_name : str
        The name of the model being used.
    llm : Any
        The chat model instance.
    token_usage_log : Any
        The token usage log used to store cumulitive tokens used during the lifetime of the ai class

    Methods
    -------
    start(system, user, step_name) -> List[Message]:
        Start the conversation with a system and user message.
    next(messages, prompt, step_name) -> List[Message]:
        Advance the conversation by interacting with the language model.
    backoff_inference(messages, callbacks) -> Any:
        Interact with the model using an exponential backoff strategy in case of rate limits.
    serialize_messages(messages) -> str:
        Serialize a list of messages to a JSON string.
    deserialize_messages(jsondictstr) -> List[Message]:
        Deserialize a JSON string into a list of messages.

    """

    def __init__(
        self, settings: GPTEngineerSettings
    ):
        """
        Initialize the AI class.

        Parameters
        ----------
        model_name : str, optional
            The name of the model to use, by default "gpt-4".
        temperature : float, optional
            The temperature to use for the model, by default 0.1.
        """
        self.settings = settings

        self.llm = self._create_chat_model()
        
        self.cache = False

    def start(self, system: str, user: str, step_name: str, max_response_length: Optional[int] = None) -> List[Message]:
        """
        Start the conversation with a system message and a user message.
        Parameters
        ----------
        max_response_length : Optional[int], optional
            The maximum length of the response generated by the AI, by default None.

        Parameters
        ----------
        system : str
            The content of the system message.
        user : str
            The content of the user message.
        step_name : str
            The name of the step.

        Returns
        -------
        List[Message]
            The list of messages in the conversation.
        """

        messages: List[Message] = [
            SystemMessage(content=system),
            HumanMessage(content=user),
        ]
        return self.next(messages, step_name=step_name, max_response_length=max_response_length)

    def next(
        self,
        messages: List[Message],
        prompt: Optional[str] = None,
        *,
        step_name: str,
        max_response_length: Optional[int] = None,
        callback = None
    ) -> List[Message]:
        """
        Advances the conversation by sending message history to LLM and updating with the response.
        Parameters
        ----------
        max_response_length : Optional[int], optional
            The maximum length of the response generated by the AI, by default None.

        Parameters
        ----------
        messages : List[Message]
            The list of messages in the conversation.
        prompt : Optional[str], optional
            The prompt to use, by default None.
        step_name : str
            The name of the step.

        Returns
        -------
        List[Message]
            The updated list of messages in the conversation.
        """
        """
        Advances the conversation by sending message history to LLM and updating with the response.
        Parameters
        ----------
        max_response_length : Optional[int], optional
            The maximum length of the response generated by the AI, by default None.
        """
        if prompt:
            messages.append(HumanMessage(content=prompt))

        # logger.debug(f"Creating a new chat completion: {messages}")

        response = None
        md5Key = messages_md5(messages) if self.cache else None
        if self.cache and md5Key in self.cache:
            response = AIMessage(content=json.loads(self.cache[md5Key])["content"])

        if not response:
            callbacks = [LogginCallbackHandler()] if self.settings.log_ai else []
            if callback:
                callbacks.append(callback)
            response = self.backoff_inference(messages, callbacks)
            if self.cache:
                self.cache[md5Key] = json.dumps(
                    {
                        "messages": serialize_messages(messages),
                        "content": response.content,
                    }
                )
        else:
            logger.debug(f"Response from cache: {messages} {response}")

        #self.token_usage_log.update_log(
        #    messages=messages, answer=response.content, step_name=step_name
        #)
        messages.append(response)
        if self.settings.log_ai:
            logger.debug(f"Chat completion finished: {messages}")

        return messages

    @backoff.on_exception(
        backoff.expo, openai._exceptions.RateLimitError, max_tries=7, max_time=45
    )
    def backoff_inference(self, messages, callbacks):
        """
        Perform inference using the language model while implementing an exponential backoff strategy.

        This function will retry the inference in case of a rate limit error from the OpenAI API.
        It uses an exponential backoff strategy, meaning the wait time between retries increases
        exponentially. The function will attempt to retry up to 7 times within a span of 45 seconds.

        Parameters
        ----------
        messages : List[Message]
            A list of chat messages which will be passed to the language model for processing.

        callbacks : List[Callable]
            A list of callback functions that are triggered after each inference. These functions
            can be used for logging, monitoring, or other auxiliary tasks.

        Returns
        -------
        Any
            The output from the language model after processing the provided messages.

        Raises
        ------
        openai.error.RateLimitError
            If the number of retries exceeds the maximum or if the rate limit persists beyond the
            allotted time, the function will ultimately raise a RateLimitError.

        Example
        -------
        >>> messages = [SystemMessage(content="Hello"), HumanMessage(content="How's the weather?")]
        >>> callbacks = [some_logging_callback]
        >>> response = backoff_inference(messages, callbacks)
        """
        return self.llm(messages=messages, config={ "callbacks": callbacks })  # type: ignore

    @staticmethod
    def serialize_messages(messages: List[Message]) -> str:
        """
        Serialize a list of messages to a JSON string.

        Parameters
        ----------
        messages : List[Message]
            The list of messages to serialize.

        Returns
        -------
        str
            The serialized messages as a JSON string.
        """
        try:
            return json.dumps(messages_to_dict(messages))
        except Exception as ex:
            logger.error(f"serialize_messages error: {messages} {ex}")
            raise ex

    @staticmethod
    def deserialize_messages(jsondictstr: str) -> List[Message]:
        """
        Deserialize a JSON string to a list of messages.

        Parameters
        ----------
        jsondictstr : str
            The JSON string to deserialize.

        Returns
        -------
        List[Message]
            The deserialized list of messages.
        """
        data = json.loads(jsondictstr)
        # Modify implicit is_chunk property to ALWAYS false
        # since Langchain's Message schema is stricter
        prevalidated_data = [
            {**item, "data": {**item["data"], "is_chunk": False}} for item in data
        ]
        return list(messages_from_dict(prevalidated_data))  # type: ignore


    def _create_chat_model(self) -> BaseChatModel:
        """
        Create a chat model with the specified model name and temperature.

        Parameters
        ----------
        model : str
            The name of the model to create.
        temperature : float
            The temperature to use for the model.

        Returns
        -------
        BaseChatModel
            The created chat model.
        """
        return OpenAI_AI(settings=self.settings).chat_completions
        """
        return ChatOpenAI(
            openai_api_key=OPENAI_API_KEY,
            openai_api_base=OPENAI_API_BASE,
            model=self.model_name,
            temperature=self.temperature,
            streaming=True,
            client=openai.ChatCompletion,
        )
        """


def serialize_messages(messages: List[Message]) -> str:
    """
    Serialize a list of chat messages into a JSON-formatted string.

    This function acts as a wrapper around the `AI.serialize_messages` method,
    providing a more straightforward access to message serialization.

    Parameters
    ----------
    messages : List[Message]
        A list of chat messages to be serialized. Each message should be an
        instance of the `Message` type (which includes `AIMessage`, `HumanMessage`,
        and `SystemMessage`).

    Returns
    -------
    str
        A JSON-formatted string representation of the input messages.

    Example
    -------
    >>> msgs = [SystemMessage(content="Hello"), HumanMessage(content="Hi, AI!")]
    >>> serialize_messages(msgs)
    '[{"type": "system", "content": "Hello"}, {"type": "human", "content": "Hi, AI!"}]'
    """
    return AI.serialize_messages(messages)


def messages_md5(messages: List[Message]):
    messageaStr = "".join(map(lambda x: x.content, messages))
    return str(hashlib.md5(messageaStr.encode("utf-8")).hexdigest())
